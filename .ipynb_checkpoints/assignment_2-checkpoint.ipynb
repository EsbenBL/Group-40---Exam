{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bc548c5757183e3aa5b100a22a9de6a6",
     "grade": false,
     "grade_id": "cell-0b3c68d95ede789f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "This is the first of two mandatory assignments which must be completed during the course. First some practical information:\n",
    "\n",
    "* When is the assignment due?: **23:59, Friday, August 24, 2018**\n",
    "* How do you grade the assignment?: You will **peergrade** each other as primary grading. \n",
    "* Must I hand-in as a group?: **yes**\n",
    "\n",
    "The assigment consist of one to three problems from each of the exercise sets you have solved so far (excluding Exercise Set 1). We've tried to select problems which are self contained, but it might be nessecary to solve some of the previous exercises in each set to fully answer the problems in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3528688d4562722e1b444037def56c53",
     "grade": false,
     "grade_id": "cell-3072780d41331083",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1117.75x1000 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "iris = iris.query(\"species == 'virginica' | species == 'versicolor'\").sample(frac = 1, random_state = 3)\n",
    "X = np.array(iris[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']])\n",
    "y = np.array(iris['species'].map({'virginica': 1, 'versicolor': -1}))\n",
    "sns.pairplot(iris, hue=\"species\", palette=\"husl\", diag_kws = {'shade': False})\n",
    "plt.show()\n",
    "\n",
    "# A very simple deterministic test-train split \n",
    "Xtrain = X[:70]\n",
    "ytrain = y[:70]\n",
    "\n",
    "Xtest = X[70:]\n",
    "ytest = y[70:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d1d1dc92d371366c3d3587f7573262fc",
     "grade": false,
     "grade_id": "cell-b0c4c7df89bbf367",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 11.1.5:** Write a function whichs loops over the training data (both X and y) using `zip`. For each row in the data, update the weights according to the perceptron rule (remember to update the bias in `w[0]`!). Set $\\eta = 0.1$.\n",
    ">\n",
    "> Make sure the loop stores the total number of prediction errors encountered underways in the loop by creating an `int` which is incremented whenever you update the weights. \n",
    ">\n",
    ">> _Hint:_ your function should return the updated weights, as well as the number of errors made by the perceptron.\n",
    ">\n",
    ">> _Hint:_ The following code block implements the function in _pseudo_code (it wont run, but serves to communicate the functionality).\n",
    ">> ```\n",
    ">> function f(X, y, W, eta):\n",
    ">>    set errors = 0\n",
    ">>\n",
    ">>    for each pair xi, yi in zip(X,y) do:\n",
    ">>        set update = eta * (yi - predict(xi, W))\n",
    ">>        set W[1:] = W[1:] + update * xi\n",
    ">>        set W[0] = W[0] + update\n",
    ">>        set errors = errors + int(update != 0) \n",
    ">>\n",
    ">>    return W, errors\n",
    ">> ```\n",
    ">\n",
    "> *Bonus:* If you completed the previous bonus exercise (for 11.1.4), calculate the accuracy on training data using the updated weights as input in the predict function. Any progress yet?\n",
    "\n",
    "You can use the following functions:\n",
    "\n",
    "```python\n",
    "def random_weights(location = 0.0, scale = 0.01, seed = 1):\n",
    "    # Init random number generator\n",
    "    rgen = np.random.RandomState(seed)\n",
    "    w = rgen.normal(loc=location, scale=scale, size= 1 + X.shape[1])\n",
    "    \n",
    "    return w\n",
    "\n",
    "def net_input(X, W): \n",
    "    return np.dot(X, W[1:]) + W[0]   # Linear product X'W + bias\n",
    "\n",
    "\n",
    "def predict(X, W):\n",
    "    linProd = net_input(X, W)\n",
    "    return np.where(linProd >= 0.0, 1, -1)    # 1(linProd > 0)\n",
    "```\n",
    "\n",
    ">\n",
    "> Make sure your function takes the arguments `X, y, W, eta` and take the name  `perceptronEpoch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weights(location = 0.0, scale = 0.01, seed = 1):\n",
    "    # Init random number generator\n",
    "    rgen = np.random.RandomState(seed)\n",
    "    w = rgen.normal(loc=location, scale=scale, size= 1 + X.shape[1])\n",
    "\n",
    "    return w\n",
    "\n",
    "def net_input(X, W): \n",
    "    return np.dot(X, W[1:]) + W[0]   # Linear product X'W + bias\n",
    "\n",
    "\n",
    "def predict(X, W):\n",
    "    linProd = net_input(X, W)\n",
    "    return np.where(linProd >= 0.0, 1, -1)    # 1(linProd > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "07299358a25773a632139da73a82b21c",
     "grade": false,
     "grade_id": "cell-2bda70ae3ab3513a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 11.1.5]\n",
    "\n",
    "def perceptronEpoch(X, y, W, eta = 0.1):\n",
    "    errors=0\n",
    "    for xi,yi in zip(X,y):\n",
    "        update=eta*(yi-predict(xi,W))\n",
    "        W[1:]=W[1:]+update*xi\n",
    "        W[0]=W[0]+update\n",
    "        errors=errors+int(update!=0)\n",
    "    return W,errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "84775f7db91f8bd98517f69793de65dc",
     "grade": true,
     "grade_id": "cell-f8b86a3cb32f287d",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "w, e = perceptronEpoch(Xtrain, ytrain, random_weights(), 0.1)\n",
    "assert len(w) == 5\n",
    "assert isinstance(e, int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "01f3dfd0d33e33e894e5e55fb1766779",
     "grade": false,
     "grade_id": "cell-aeb7d747c39cfb45",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 11.1.6:** Write a function which repeat the updating procedure (calls the function) you constructed in 11.1.5 for `n_iter` times by packing the whole thing in a loop. Make sure you store the number of errors in each iteration in a list. \n",
    ">\n",
    "> Plot the total errors after each iteration in a graph.\n",
    ">\n",
    ">> _Hint:_ Make sure you dont reset the weights after each iteration.\n",
    ">\n",
    ">> _Hint:_ Once again some pseudocode:\n",
    ">> ```\n",
    ">> function g(X, y, n_iter):\n",
    ">>     set eta = 0.1\n",
    ">>     set weights = random_weights()\n",
    ">>     set errorseq = list()\n",
    ">>\n",
    ">>     for each _ in range(n_iter):\n",
    ">>         weights, e = f(X, y, W, eta) \n",
    ">>         errorseq.append(e)\n",
    ">>\n",
    ">>     return weights, errorseq\n",
    ">> ```\n",
    "\n",
    "Please make sure that your function is named `Perceptron` and takes the arguments `X, y, n_iter, eta`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8411b72e1fb5b0dd4b9b0f770448eee4",
     "grade": false,
     "grade_id": "cell-b1c61239647f50bc",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 11.1.6]\n",
    "\n",
    "def Perceptron(X, y, n_iter = 50, eta = 0.1):\n",
    "    weights=random_weights()\n",
    "    errorseq=list()\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        weights,e = perceptronEpoch(X,y,weights,eta)\n",
    "        errorseq.append(e)\n",
    "        \n",
    "    return weights, errorseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "aa2d9c611f4bd6f8c0e2d44bef5254ae",
     "grade": true,
     "grade_id": "cell-cb48d432ba1d1777",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "trained_w, errorseq = Perceptron(Xtrain, ytrain, 50, 0.1)\n",
    "assert len(trained_w) == 5\n",
    "assert len(errorseq) == 50\n",
    "assert all(isinstance(i, int) for i in errorseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "38e2c8f68e3d93b14688505fc30f5a1f",
     "grade": false,
     "grade_id": "cell-09ee8b8bfa7d20ae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problems from exercise set 12\n",
    ">Get the required data by running the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b07dc7d9d0d294f64c8d33f21cab26af",
     "grade": false,
     "grade_id": "cell-146e95214415be31",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cal_house = fetch_california_housing()    \n",
    "X = pd.DataFrame(data=cal_house['data'], \n",
    "                 columns=cal_house['feature_names'])\\\n",
    "             .iloc[:,:-2]\n",
    "y = cal_house['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4cc189ad95b145598bbe3dec8396604c",
     "grade": false,
     "grade_id": "cell-d873b3cec5d922df",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex.12.2.1**: Generate interactions between all features to third degree, make sure you **exclude** the bias/intercept term. How many variables are there? Will OLS fail? \n",
    ">\n",
    "> After making interactions rescale the features to have zero mean, unit std. deviation. Should you use the distribution of the training data to rescale the test data?  \n",
    ">\n",
    ">> *Hint 1*: Try importing `PolynomialFeatures` from `sklearn.preprocessing`\n",
    ">\n",
    ">> *Hint 2*: If in doubt about which distribution to scale, you may read [this post](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i).\n",
    "\n",
    "Name you transformed training data set `X_train2` and your test data set `X_test2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6a22d591e5e6e96c227abfc0db9081a9",
     "grade": false,
     "grade_id": "cell-08fe4914c0e8cad3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.2.1]\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "poly=PolynomialFeatures(degree=3,include_bias=False,interaction_only=True)\n",
    "scaler=StandardScaler()\n",
    "\n",
    "x_train_data=poly.fit_transform(X_train)\n",
    "X_train2= scaler.fit_transform(x_train_data)\n",
    "\n",
    "x_test_data=poly.transform(X_test)\n",
    "X_test2 =scaler.transform(x_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "21efb0b4c654c5db093f3d2d892e60ae",
     "grade": true,
     "grade_id": "cell-549143d7ba521f0e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 'PolynomialFeatures' in dir()\n",
    "assert np.allclose(X_train2.mean(axis=0), 0)\n",
    "assert np.allclose(X_train2.std(axis=0), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6d1a7b10230e727661ad32525e1f5f8a",
     "grade": false,
     "grade_id": "cell-f5f3f01a9c903af4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex.12.2.2**: Estimate the Lasso model on the train data set, using values of $\\lambda$ in the range from $10^{-4}$ to $10^4$. For each $\\lambda$  calculate and save the Root Mean Squared Error (RMSE) for the test and train data. \n",
    ">\n",
    ">> *Hint*: use `logspace` in numpy to create the range.\n",
    "\n",
    "Please name your root mean square function `rmse` which takes the arguments `y_pred, y_true`.\n",
    "Please store you results in a list named `output` which is a list of lists. `output` should take the following form `[[<lambda parameter>, <root mean squared error for training data>, <root mean squared error for test data>]]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2ab47e1228930f5a9f79daee40f29cd3",
     "grade": false,
     "grade_id": "cell-9fa702c4cfe407eb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0001, 0.7240143501909345, 5.015620034526878],\n",
       " [0.00026366508987303583, 0.7243110452022222, 4.451105691280012],\n",
       " [0.0006951927961775605, 0.7258128087101818, 4.0757591201900985],\n",
       " [0.0018329807108324356, 0.7312124679756509, 4.317473540787815],\n",
       " [0.004832930238571752, 0.7448902194811705, 2.768409726770694],\n",
       " [0.012742749857031334, 0.7695067186794392, 0.7788829809856797],\n",
       " [0.03359818286283781, 0.8011516876777676, 0.809857634646904],\n",
       " [0.08858667904100823, 0.8108969369179121, 0.8178255445495782],\n",
       " [0.23357214690901212, 0.8467540437549512, 0.8502175095991171],\n",
       " [0.615848211066026, 1.0362260118324642, 1.0412001100155583],\n",
       " [1.623776739188721, 1.1502742926333995, 1.1575985329014546],\n",
       " [4.281332398719396, 1.1502742926333995, 1.1575985329014546],\n",
       " [11.288378916846883, 1.1502742926333995, 1.1575985329014546],\n",
       " [29.763514416313132, 1.1502742926333995, 1.1575985329014546],\n",
       " [78.47599703514607, 1.1502742926333995, 1.1575985329014546],\n",
       " [206.913808111479, 1.1502742926333995, 1.1575985329014546],\n",
       " [545.5594781168514, 1.1502742926333995, 1.1575985329014546],\n",
       " [1438.44988828766, 1.1502742926333995, 1.1575985329014546],\n",
       " [3792.690190732246, 1.1502742926333995, 1.1575985329014546],\n",
       " [10000.0, 1.1502742926333995, 1.1575985329014546]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Answer to Ex. 12.2.2] \n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from math import sqrt\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "def rmse(y_pred, y_true):\n",
    "    return sqrt(mse(y_pred, y_true))\n",
    "\n",
    "lambdas=np.logspace(-4,4,20)\n",
    "train=[]\n",
    "test=[]\n",
    "\n",
    "for lambda_ in lambdas:\n",
    "    lasso_m = Lasso(alpha=lambda_,random_state=1000)\n",
    "    lasso_m.fit(X_train2,y_train)\n",
    "    y_pred_train = lasso_m.predict(X_train2)\n",
    "    y_pred_test=lasso_m.predict(X_test2)\n",
    "    train.append(rmse(y_pred_train, y_train))\n",
    "    test.append(rmse(y_pred_test,y_test))\n",
    "\n",
    "df_train = pd.DataFrame(train, index = lambdas)\n",
    "df_train.columns = ['Train']\n",
    "df_test = pd.DataFrame(test, index = lambdas)\n",
    "df_test.columns = ['Test']\n",
    "MSE_df = pd.concat([df_train, df_test], axis=1)\n",
    "df_with_index = MSE_df.reset_index()\n",
    "output = df_with_index.values.tolist()\n",
    "output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "67cda383bea4276cf0f3f56c84851d16",
     "grade": true,
     "grade_id": "cell-bacb9951b20a7eb1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(output) == 20\n",
    "assert all(len(i) == 3 for i in output)\n",
    "assert np.allclose([i[0] for i in output], np.logspace(-4, 4, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "054da1a320071d52c1cbc61cdd740db4",
     "grade": false,
     "grade_id": "cell-e138853f6783d024",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex.12.2.3**: Make a plot with on the x-axis and the RMSE measures on the y-axis. What happens to RMSE for train and test data as $\\lambda$ increases? The x-axis should be log scaled. Which one are we interested in minimizing? \n",
    "\n",
    "> Bonus: Can you find the lambda that gives the lowest MSE-test score?\n",
    "\n",
    "Please store your DataFrame in a variable named `MSE_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4d19dd215e35b51dc8254eb760c75b40",
     "grade": false,
     "grade_id": "cell-6044e3834a35b132",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0.5, 'RMSE'), Text(0.5, 0, 'Lambda')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [Answer to Ex. 12.2.3]\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "%matplotlib inline\n",
    "\n",
    "ax = MSE_df.plot(logx=True,label=\"RMSE\", use_index=True)\n",
    "ax.set(xlabel = 'Lambda', ylabel = 'RMSE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b92cb5fefe7ae11d81a2c79c67194e41",
     "grade": true,
     "grade_id": "cell-6619297b5b1d6256",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert MSE_df.shape[0] == 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0e866a6e23f26918c28eea526441986f",
     "grade": false,
     "grade_id": "cell-c4ac314e549df768",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problems from exercise set 13\n",
    "\n",
    "> **Ex. 13.1.3:**\n",
    "Run a Lasso regression using the Pipeline from `Ex 13.1.2`. In the outer loop searching through the lambdas specified below. \n",
    "In the inner loop make 5 fold cross validation on the selected model and store the average MSE for each fold. Which lambda gives the lowest test MSE?\n",
    ">\n",
    "You are supposed to append list elements to the list `mses` where the elements are `[the average of the mean squared errors of the kfolded TRAINING data with the specific lambda, the average of the mean squared errors of the kfolded TEST data with the specific lambda, the SPECIFIC LAMBDA]`. The length of the list should be 12 because you've got 12 different lambda parameters.\n",
    "\n",
    ">\n",
    ">> *Hint:* `KFold` in `sklearn.model_selection` may be useful.\n",
    ">\n",
    "> This code will give you the required data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "137e9cd57cd7ab789a7cf46b111b86c9",
     "grade": false,
     "grade_id": "cell-cd167a6ab52b4e77",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "cal_house = fetch_california_housing()    \n",
    "X = pd.DataFrame(data=cal_house['data'], \n",
    "                 columns=cal_house['feature_names'])\\\n",
    "             .iloc[:,:-2]\n",
    "y = cal_house['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dc79c6dd4f2e8b131301e9cac81535b8",
     "grade": false,
     "grade_id": "cell-df6080fe4c379a97",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0001, 41.13380096553595, 41.13380096553595],\n",
       " [0.0005336699231206312, 32.37421594835057, 32.37421594835057],\n",
       " [0.002848035868435802, 7.162217102695318, 7.162217102695318],\n",
       " [0.01519911082952933, 0.5929383951494878, 0.5929383951494878],\n",
       " [0.08111308307896872, 0.6533091823155663, 0.6533091823155663],\n",
       " [0.43287612810830617, 0.8719801031205575, 0.8719801031205575],\n",
       " [2.310129700083158, 1.3170635781546818, 1.3170635781546818],\n",
       " [12.32846739442066, 1.3170635781546818, 1.3170635781546818],\n",
       " [65.79332246575683, 1.3170635781546818, 1.3170635781546818],\n",
       " [351.11917342151344, 1.3170635781546818, 1.3170635781546818],\n",
       " [1873.8174228603868, 1.3170635781546818, 1.3170635781546818],\n",
       " [10000.0, 1.3170635781546818, 1.3170635781546818]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Answer to Ex. 13.1.3]\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=2)    \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=1/2, random_state=2)\n",
    "\n",
    "lambdas =  np.logspace(-4, 4, 12) # define lambda - devide into 12 lambdas between 10**-4 and 10**4\n",
    "kfolds = KFold(n_splits=5) # define nr. of folds\n",
    "\n",
    "mse_val = []\n",
    "mse_test = []\n",
    "mse_val_avr = []\n",
    "mse_test_avr = []\n",
    "lambda_list = []\n",
    "\n",
    "#Outer-loop (Lambdas)\n",
    "for lambda_ in lambdas:\n",
    "#Inner-loop (Folds)\n",
    "    mse_val_ = []\n",
    "    mse_test_ = []\n",
    "    \n",
    "    # Split the dataset into 5 folds, and loop through each fold. \n",
    "    # In the iteration, the fold in question acts as the training-set. \n",
    "    for train_idx,val_idx in kfolds.split(X_dev, y_dev):\n",
    "    # train model and compute MSE on test fold\n",
    "        pipe_Lasso_CV = make_pipeline(PolynomialFeatures(include_bias=False, degree = 3), \n",
    "                        StandardScaler(with_mean = 0, with_std = True ),\n",
    "                        Lasso(alpha = lambda_, random_state=1))\n",
    "        # Assign X_train & y_train by extracting elements if the folds from X_dev and y_dev\n",
    "        X_train, y_train = X_dev.iloc[train_idx], y_dev[train_idx]\n",
    "        # Assign X_val & y_val by extracting elements of the folds from X_dev and y_dev\n",
    "        X_val, y_val = X_dev.iloc[val_idx], y_dev[val_idx] \n",
    "        # Fit Lasso-model to training data\n",
    "        pipe_Lasso_CV.fit(X_train, y_train)   \n",
    "        #Calculate and append the mse into mse_val list:\n",
    "        mse_val_.append(mse(pipe_Lasso_CV.predict(X_val), y_val))\n",
    "        ##Calculate and append the mse into mse_test list:\n",
    "        mse_test_.append(mse(pipe_Lasso_CV.predict(X_test), y_test))\n",
    "        \n",
    "    mse_val_avr.append(sum(mse_val_)/len(mse_val_))\n",
    "    mse_test_avr.append(sum(mse_val_)/len(mse_val_))\n",
    "    mse_val.append(mse_val_)\n",
    "    mse_test.append(mse_test_)\n",
    "\n",
    "\n",
    "\n",
    "df_val = pd.DataFrame(mse_val_avr, index = lambdas)\n",
    "df_test = pd.DataFrame(mse_test_avr, index = lambdas)\n",
    "df_join = pd.concat([df_test, df_val], axis=1)\n",
    "\n",
    "df_with_index = df_join.reset_index()\n",
    "mses = df_with_index.values.tolist()\n",
    "mses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6c88aa05b2fc68b6cf17612d82f08a55",
     "grade": true,
     "grade_id": "cell-37fc3f96d84b608d",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(mses) == 12\n",
    "assert all(len(i) == 3 for i in mses)\n",
    "assert all(isinstance(i[0], float) for i in mses)\n",
    "assert all(isinstance(i[1], float) for i in mses)\n",
    "assert all(isinstance(i[2], float) for i in mses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "058d0a12d7956bacdf8d91bdae42a272",
     "grade": false,
     "grade_id": "cell-19693c98308793a4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise Set 15: Text Classification and Sentiment Analysis\n",
    "\n",
    "*Morning, August 21, 2019*\n",
    "\n",
    "In this Exercise Set you will practice using two basic text classification methods: rule- and machine learning-based. The exercise has XX parts:\n",
    "\n",
    "1. Implement a lexical look-up method.\n",
    "2. Apply pre-packaged rulebased dictionaries.\n",
    "3. Train a simple baseline machine learning classifier.\n",
    "\n",
    "In the end, you will then compare the results of these approaches.\n",
    "\n",
    "First, load our standard stuff and import the following modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8152ecc95e956f3bdcd65b8ad8176e60",
     "grade": false,
     "grade_id": "cell-14bb71590e016bf5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Standard stuff:\n",
    "import numpy as np, seaborn as sns, pandas as pd\n",
    "## For text classification:\n",
    "import nltk, nltk.sentiment, sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "46c79b3cf5573d1c7658947a215f341a",
     "grade": false,
     "grade_id": "cell-f8b43193d368738a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 15 Part 1: Implementing your own Lexical Lookup method\n",
    "There are many curated dictionaries and lexicons online for all sorts of topics (see for instance this project: https://hedonometer.org/index.html where the lexicons behind it can be downloaded here: https://github.com/andyreagan/hedonometer/blob/master/hedonometer/static/hedonometer/labMT1.txt). For this exercise we will use the following list of positive and negative words (positive:http://ptrckprry.com/course/ssd/data/negative-words.txt ; negative: http://ptrckprry.com/course/ssd/data/positive-words.txt) compilled by Hu and Liu. \n",
    "\n",
    "We will use the following dataset (a random sample of the trustpilot review data we collected in exercise 8) to practice on.\n",
    "\n",
    "> **Ex. 15.1.1:**  Load it like this (remember to import the `request` package): \n",
    "\n",
    "```python \n",
    "# download data\n",
    "path2data = 'https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv'\n",
    "df = pd.read_csv(path2data)```\n",
    "\n",
    "The important columns are ***reviewBody*** containing the text, and the ***reviewRating_ratingValue*** containing the rating / stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "8e78b05d37f0bfda76cf4848f186f88a",
     "grade": false,
     "grade_id": "cell-f53708052624c3e8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 15.1.1]\n",
    "path2data = 'https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv'\n",
    "df = pd.read_csv(path2data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e20bab1acbfe2d9f9efa47a31ba5cb49",
     "grade": true,
     "grade_id": "cell-761d11fc22d7f54a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(df, pd.DataFrame)\n",
    "assert df.shape == (10000, 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8736e4623a87e77960b7d42d8df4f92b",
     "grade": false,
     "grade_id": "cell-edcc60866444c05f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "> **Ex. 15.1.2:** Next, we should download and prepare the dictionaries.\n",
    "1. Download the lists using python's `requests.get()`. The lists are documents of words separated by new line (which is the '\\n' character). \n",
    "2. Make sure to remove the comment section in the top by splitting at the right place. \n",
    "3. `.split()` these documents into words.\n",
    "4. Convert them into sets (using the `set()`-command) and assign these to two variables (e.g. ***positive*** and ***negative***). \n",
    "\n",
    "*Hint*: You can do all of the above in one line of code per list.\n",
    "\n",
    "Please make sure your positive set is named `positive` and your negative set `negative`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e0551ff331b373522614fbc62c6eaecc",
     "grade": false,
     "grade_id": "cell-4388ca83d13bea29",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex. 15.1.2]\n",
    "import requests\n",
    "url_pos='http://ptrckprry.com/course/ssd/data/positive-words.txt'\n",
    "url_neg='http://ptrckprry.com/course/ssd/data/negative-words.txt'\n",
    "pos= requests.get(url_pos).text.split(\"\\n\")[35:]\n",
    "neg= requests.get(url_neg).text.split(\"\\n\")[35:]\n",
    "positive=set(pos)\n",
    "negative=set(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "41f2fcb37dbac1049e32e705ce744642",
     "grade": true,
     "grade_id": "cell-7a3e45ee655276f6",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(positive, set)\n",
    "assert isinstance(negative, set)\n",
    "assert len(positive) == 2007\n",
    "assert len(negative) == 4784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5558022f190b076ae24760cc7f931e3a",
     "grade": false,
     "grade_id": "cell-f27e2d1aaf8c756b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we need to prepare the trustpilot reviews, our documents to be analyzed. This means lowercasing and tokenizing them to match the format that our dictionary comes in.\n",
    "\n",
    "> **Ex. 15.1.3:** Define a function `preprocessing(string)`, that takes in a string and returns a list of words. The function should do the following: \n",
    "1. lowercases the string using the `.lower()` command.\n",
    "2. tokenize the words using the `nltk.tokenize.TweetTokenizer()` which is good for social media type user content (i.e. emojiies and more free use of punctuation and commas. \n",
    "3. return tokenized documents.\n",
    "\n",
    "*Hint*: You can first initialize the tokenizer (outside the preprocessing function), and then you use the `.tokenize()` method.\n",
    "\n",
    "Please make sure your tweettokenizer instance is store in a variable named `tokenizer` and your preprocessing function should take the name `preprocessing` that takes the argument `string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4cf9eb97e4206c4963cb5054c22c28b5",
     "grade": false,
     "grade_id": "cell-5cb74325fbd9e541",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex 15.1.3]\n",
    "import nltk, nltk.sentiment\n",
    "tokenizer=nltk.tokenize.TweetTokenizer()\n",
    "def preprocessing(string):\n",
    "    w=tokenizer.tokenize(string)\n",
    "    words=[word.lower() for word in w]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "93f1ee3cfdd67219e43e419496723326",
     "grade": true,
     "grade_id": "cell-25d6a67cbe886a53",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tweet = \"Denmark is a very special country with incredible people, but based on Prime Minister Mette Frederiksen’s comments, that she would have no interest in discussing the purchase of Greenland, I will be postponing our meeting scheduled in two weeks for another time....\"\n",
    "assert isinstance(tokenizer, nltk.tokenize.TweetTokenizer)\n",
    "assert preprocessing(tweet)[:3] == ['denmark', 'is', 'a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3ddd5fb0a268fd99060f6aa2850422fe",
     "grade": false,
     "grade_id": "cell-543a959a83c8613c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    ">**Ex 15.1.4:** Apply the preprocessing function to all the documents (i.e. our review texts in the column ***reviewBody***).\n",
    "\n",
    "*Hint:* Use the `.apply()` method.\n",
    "\n",
    "Please store your processed pandas series in a variable named `documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4a63deceb47db57a27c4d24dc2fa0099",
     "grade": false,
     "grade_id": "cell-4c0948e9ef6fb6f0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer to Ex 15.1.4]\n",
    "documents=df[\"reviewBody\"].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf2061c219c6e93544a14569a532f8a6",
     "grade": true,
     "grade_id": "cell-f6ae792632c1939f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(documents, pd.Series)\n",
    "assert documents.shape == (10000,)\n",
    "assert documents.apply(lambda x: isinstance(x, list)).all()\n",
    "assert documents.apply(lambda x: all(isinstance(i, str) for i in x)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "857c7ec383504da11387b620507416a9",
     "grade": false,
     "grade_id": "cell-7672d724222b2af2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now, we need to match the words in our dictionaries to the tokenized documents.\n",
    ">**Ex 15.1.5:** Define a function `count_dictionary(document,dictionary)` that takes a tokenized document and a set of words (i.e. the dictionaries we loaded in Ex 15.1.2) and counts the number of matches. The function should do the following:\n",
    "1. Filter the words not in the dictionary. \n",
    "*Hint*: you can use a list comprehension or for loop and `if word in` condition. \n",
    "2. Return the length of the filtered document.\t\t\n",
    "\n",
    "Please make sure your function is named `count_dictionary` and takes the arguments `tokenized_doc, dictionary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0278a49b836fd6257eb348d2272c62f8",
     "grade": false,
     "grade_id": "cell-8c891f460308f996",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer Ex 15.1.5]\n",
    "#def count_dictionary(tokenized_doc,dictionary):\n",
    "def count_dictionary(tokenized_doc,dictionary):\n",
    "    hits=[word for word in tokenized_doc if word in dictionary]\n",
    "    return len(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "17ad55abb966122f1560790bbe1c7e13",
     "grade": true,
     "grade_id": "cell-676422cc3af32793",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "in_positive = ['precisely',\n",
    " 'futurestic',\n",
    " 'indebted',\n",
    " 'vouchsafe',\n",
    " 'freedoms',\n",
    " 'grandeur',]\n",
    "\n",
    "not_in_positive = [\n",
    "    \"wascs\",\n",
    "    \"cpajos\"\n",
    "]\n",
    "\n",
    "l = in_positive + not_in_positive\n",
    "assert count_dictionary(l, positive) == len(in_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b604d1345f69c5afabf4d2112bd4c225",
     "grade": false,
     "grade_id": "cell-1455c6b7aeb3cc79",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    ">**Ex 15.1.6:** Define two new columns (***positive_liu***, ***negative_liu***) in the dataframe applying the count_dictionary function to all tokenized documents with the positive and negative set as input. \n",
    "\n",
    "*Hint:* the `.apply()` method allows you to input named arguments matching your `count_dictionary()` function to input the sentiment dictionary. \n",
    "\n",
    "Please make sure that you have stored the number of words in the dataframed named `df` in the columns `positive_liu` and `negative_liu` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3225bbd3c1176ca492bc7e6914e99b4f",
     "grade": false,
     "grade_id": "cell-88fd0b69ba8eef91",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# [Answer Ex 15.1.6]\n",
    "df[\"positive_liu\"]=[count_dictionary(i, positive) for i in documents]\n",
    "\n",
    "df[\"negative_liu\"]=[count_dictionary(i, negative) for i in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "76a45e8e31e758016903133029c90bc3",
     "grade": true,
     "grade_id": "cell-8421b1dfee394d0f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert df['positive_liu'].apply(lambda x: isinstance(x, (int, float))).all()\n",
    "assert df['negative_liu'].apply(lambda x: isinstance(x, (int, float))).all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
