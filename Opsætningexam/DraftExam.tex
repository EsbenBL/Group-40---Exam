\documentclass[12pt,a4paper]{article}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\onehalfspacing
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{pst-tree}
\usepackage{tikz}
\usetikzlibrary{trees}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage[top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage{enumerate} 
\usepackage{amssymb}
\usepackage{csquotes}
\usepackage{fancyhdr}
\usepackage{times}
\usepackage{fancyhdr}
\usepackage[bottom]{footmisc}
\pagestyle{fancy}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{lscape}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
\node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
\DeclareMathSizes{12}{12}{10}{10}
\interfootnotelinepenalty=10000
\usepackage{dsfont}
\usepackage{mathtools}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{multirow, booktabs}
\usepackage[final]{pdfpages}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{booktabs,siunitx}
\usepackage{float}
\usepackage{caption}
\lhead{}
\rhead{}
\chead{Social Data Science, Summer 2019.}


\begin{document}
\includepdf[pages=-]{forside.pdf}
\newpage
\onehalfspacing

\tableofcontents
\newpage
\section{Introduction}
This report depicts a research performed regarding active Danish real-estate property valuations from 2016-2019. Since 2011, the yearly number of real estate properties sold has slightly increased, with 2017 being the year with most sold properties in 10 years \href{https://www.dst.dk/da/Statistik/nyt/NytHtml?cid=27979}{(Danmarks Statistik, 2018)}.  The price of real estate properties is also rising through 2019 \href{https://www.dst.dk/da/Statistik/nyt/NytHtml?cid=28741}{(Danmarks Statistik, 2019)}. Although this generally depicts a willingness of buyers to pay more for real-estate property, and higher valuations from agents, a closer analysis of current and active valuations add a local view of which conditions contribute to the valuation of a real-estate property. \newline
\newcommand{\source}[1]{\caption*{Source: {#1}} }\begin{figure}[H]
\centering
\caption{}
\includegraphics[scale=0.4]{123.png}
\source{Own creation, with data from \href{https://www.boliga.dk}{Boliga.dk}}
\end{figure}

Figure 1 shows the average square meter price valuation of active offers per municipality in Denmark. The maximum square meter price being approximately 8 times higher in the municipality of Gentofte, than Lolland, the municipality with lowest average square meter price valuations. A tendency visualized in the figure, is that average square meter price valuations are higher in in highly populated municipalities and in suburban areas surrounding Copenhagen. \newline

The valuations included in the research are collected from one of Denmarks largest online real-estate websites named Boliga.dk. The Boliga data contains approximately 66,000 active offerings with valuations ranging from 15,000 to 85 mil.dkk. This research paper intends to analyze real estate price valuations and which effects geo- and sociodemographic criteria affect valuations, using machine learning models. Our research questions is as follows: 
\begin{center}
 \textit{Which features are most relevant for predicting evaluations of real estate properties in Denmark?}
\end{center}
This research paper contains a section describing the construction of research data and assesses the choices made in gathering meaningful features for the machine learning model. Also, a section regarding the choice and optimization of machine learning models is included, where the intention is to provide insights on our progress on finding the optimal model. As a result, a preferable machine learning model is chosen with a discussion of its usability. 

\section{Literature Review}
Within data social science there exist widely differing definitions of big data and machine learning. This section seeks to clarify the use of these terms within the scope of this paper.
\paragraph{On big data \& machine learning\newline}
Historically, big data has been a term reserved for data that was unable to be processed by extant software. However, recent increases in computational power has enabled data processing of hitherto unheard of quantities of data\footnote{Lazer, David and Jason Radford (2017) \textit{Data ex Machina. Introduction to Big Data}. Annual Review of Sociology}. Today, big data is no longer too cumbersome to analyse. Matthew Salganik\footnote{Salganik, Matthew J. (2018) \textit{Bit by Bit Bit - Social research in the digital age.}} instead mentions ten typical characteristics of big data. Salganik's tentative definition suggests that big data is not as single entity, but includes many different types of systems. Among the most important features of Salganik's definition of big data is that the data has a high frequency of observations and is continuously being generated. Since the data is always-on it is also drifting, meaning that the structure and population it represents is ever-changing. It is therefore important to understand that big data is not a naturally occurring system, but driven by the engineered purpose of the system. This algorithmic confounding forces the scientist to be careful regarding any observed human behaviour that is extracted from a single digital system.\newline
In the analysis of big data it is often useful to employ machine learning as it can identify and predict various nonlinear relationships in big data sets, that otherwise would remain hidden. Data scientists' have for the most part optimised the predictive capabilities of the algorithm applied, and as a consequence they have often ignored or trivialised machine learning's potential in causal modelling\footnote{Varian. Hal R. \textit{Big data: New tricks for econometrics}}.\newline
The technical and theoretical challenges faced by big data and machine learning research are important to consider, when employing the tools they provide. One of the major discussions revolve around machine learning's predictive capabilities. Chris Anderson\footnote{Anderson, Chris (2008) \textit{The end of theory: The data deluge makes the scientific method obsolete}} (2008) argues that since the computing power and the scale of data has increased exponentially, our reliance of scientific models could become obsolete. Instead of focusing on the theoretical implications of observations, scientists should, according to Anderson, focus on the statistical outputs: In the age of big data, correlation perhaps should supersede causality and consequently social data scientists should not try to develop coherent models or unified theories to explain a social phenomenon.\newline
However, many social data scientists argue against this point of view. Justin Grimmer\footnote{Grimmer, Justin (2015) \textit{We are all social scientists now: how big data, machine learning, and causal inference work together}} claims, that correlations extracted from big data cannot stand alone. Large quantities of data is not sufficient to make scientifically valid causal inferences. It requires a rigorous research design and clear theoretical assumptions, in order to yield scientifically accurate estimates. Indeed social sciences greatest contribution to big data research, comes from the organized framework provided by rigorously tested theory\footnote{Einav, Liran and Jonathan Levin (2014) \textit{Economics in the age of big data}}.\newline
The contribution from the social sciences to machine learning help create new methods that will be able to utilize the strengths of machine learning to help solve causal inference problems within the framework of a well defined theory\footnote{Athey, Susan (2018) 'The Impact of Machine Learning on Economics' in Ajay Agrawal et al. (eds) \textit{The Economics of Artificial Intelligence: An Agenda}}. These new approaches could help define what variables to manipulate and how to properly use machine learning within the framework of theoretical assumptions. Ultimately, big data and machine learning could increase the scope of the social scientist's field, not only by delivering new data and methods, but by helping the social scientist to focus on new questions\footnote{Mullainathan, Sendhil, and Jann Spiess (2017) \textit{Machine Learning: An Applied Econometric Approach}}.

\section{Data Description \& Ethics}

\subsection{Ethical Considerations in the Current Research Project}
In the current paper the appropriate care and consideration has been given to the ethical concerns regarding the scraping, processing, and the presentation of the data. Drawing upon the European Comissions\footnote{European Commission (2018). \textit{Ethics in Social Sciences and the Humanities}} ethical guidelines and principles for ethical conduct in social data science, the potential harm to users of Boliga's web-page were carefully considered. As a step to prevent the mosaic effect and in an effort to anonymize the scraped data only aggregated data will be presented in this paper, so that no single observation can be identified from the analysed data.\newline
No informed consent has been obtained from the users of the site, prompting us to consider the consequences of the lack thereof, as informed consent is paramount to the proper, ethical conduct in social science. However, sometimes, as in this instance, informed consent can be logistically impossible to collect from all participants in the study. Salganik mentions that informed consent for everything is an ideal, but in practice impossible to obtain and researchers should instead strive to follow an alternative rule, that he describes as: "some form of consent for most things." \footnote{Salganik, Matthew J. (2018) \textit{Bit by Bit Bit - Social research in the digital age.} p.303} Adhering to this, more complex  understanding of the practicality of informed consent, we chose to contact Boliga to inform them of our intent to scrape their website and use the data in an educational context. Boliga responded positively to our inquiry, which we took as informed consent from a third party on behalf of the participating users in our study.\newline
In considering the legal ramifications of our research and to make sure we adhere to the seven principles of GDPR, and other appropriate legislation and legal contracts, we consulted the general guidelines introduced by the Consumer Data Research Center. In particular we noted that we are justified to collect and use the data on the lawful basis of legitimate interests. Furthermore, we consulted with Boliga's Terms and Conditions to avoid any legal ramifications and the appropriate contractual terms of interest can be seen in \href{https://www.boliga.dk/vilkaar-og-betingelser}{§10 Terms and Conditions}. In order to comply with these terms we refrained from burdening their website's performance by implementing a time.sleep function, which causes each scraping iteration to pause for 0.5 seconds before commencing on scraping the next page (see appendix XX). Furthermore, Boliga prohibits the use of automated scrapers and bots, which we did not do, as our scraping was done in a specified time-frame and we did not automate the procedure to be done on multiple occasions.
\subsection{Data Scraping Process}
In the following paragraphs our scraping efforts will be described. The scrapers can be examined in the attached Jupyter Notebooks labelled \textcolor{red}{XX and XX.} 
\paragraph{\href{https://www.boliga.dk}{Boliga}\newline}
Our data comes from \href{https://www.boliga.dk}{Boliga.dk}, the largest independent online web-portal for real-estate sales in Denmark and has access to unique features such as "liggetid", price-development and access to BBR - the Danish Building and Housing Register \footnote{\href{https://www.boligagruppen.dk}{www.boligagruppen.dk}}. Giving us unique insights into the pricing of real-estate in all 98 municipalities of Denmark. At the time of scraping 65,950 properties were for sale.\newline
The scraping process was conducted on Friday the 23rd of August 2019. We had advised Boliga of our intent to collect data from their website via our scraper in an effort to identify ourselves and our intent\footnote{Shiab, Nael (2015) \textit{On the Ethics of Web Scraping and Data Journalism}. Global Investigative Journalism Network.}. In order to scrape the data of interest we familiarized ourselves with the HTML-structure of Boliga. On the basis of these insights we constructed a code, which were able to scrape every page, containing information pertaining the currently listed real-estates on Boliga. The scraper requested all information available from each individual page, which surmounted to 1,319 URL requests. \newline
For each URL, 34 features and the target variable (price) was collected. Table \textcolor{red}{XX} provides an overview of the features with a short description, and whether the feature has been dropped or saved for later usage. There are three reasons for a feature to be dropped:\newline
1. The feature does not act with independent characteristics according to the research,\newline
2. The feature contains insufficient data,\newline
3. The feature is poorly formatted and cannot efficiently be recreated. 
\vspace*{10px} \newline
\textit{Features Obtained:} \newline
\begin{tabular}{c c}
Continuous: & basementSize, buildYear, ownersExspenses, lotSize	, price, rooms, size  \\	
\end{tabular}\newline 
\begin{tabular}{c c}
Categorical: & ForClosure, Type, Municipality, lotSize	, price, rooms, size  \\	
\end{tabular}
\paragraph{\href{https://www.hvorlangterder.dk}{Hvorlangterder.dk}\newline}
\href{https://www.hvorlangterder.dk}{hvorlangterder.dk} returns distances from a given address, to conveniences such as supermarkets, hospitals and schools.   
The scraping of \href{https://www.hvorlangterder.dk}{hvorlangterder.dk} was achieved by writing a function that took in the GPS-coordinates gathered from Boliga. It constructed a URL (65,950 URL request), \textcolor{red}{which was scraped and the json response was returned as a dictionary of distances to the points of interest. The values from each key in the dictionary was then extracted as a new column in a Pandas DataFrame}.\newline
As Jupyter performs poorly at running long asynchronous tasks and estimated a running time of 18 hours this procedure was run in Visual Studio.
 \vspace*{10px} \newline
\textit{Features obtained:}\newline
\begin{tabular}{c c}
Distances to: & lake, forest, doctor, supermarket,	school, daycare, hospital, train \\	
\end{tabular}\newline 
\begin{tabular}{c c}
\qquad \qquad \qquad \qquad &  pharmacy, library, coast, junction \\	
\end{tabular}
\paragraph{Social and economic factors\newline}
Social and economic factors on municipality level is collected from \href{https://statistik.politi.dk/QvAJAXZfc/opendoc.htm?document=QlikApplication%2F2999_Public\%2FPublic_IndsatsResultater.qvw}{statistik.politi.dk} and  
\href{https://www.dst.dk/da/Statistik/emner/befolkning-og-valg}{Danmarks Statistik} respectively. These factors include income, reported crime, level of highest completed education etc. These are transformed into ratios, by taking the total population in a given municipality into account.   
 \vspace*{5px} \newline
\begin{tabular}{c c}
\textit{Features obtained:}: & unemployment\_relative, primary\_school\_educ, high\_school\_educ \\	
\end{tabular}\newline 
\begin{tabular}{c c}
\small (relative to population) & vocational\_educ,	 SHE, 	MHE, bachelors\_degree  \\	
\end{tabular}\newline 
\begin{tabular}{c c}
\qquad \qquad \qquad \qquad \quad & LHE, avg\_municipal\_income\_2017, Total\_reported\_crime\\	
\end{tabular}\newline 
\begin{tabular}{c c}
\qquad \qquad \qquad \qquad \quad & Population\_in\_urban\_development  , Socioeconomic\_index \\	
\end{tabular}\newline 
\begin{tabular}{c c}
\qquad \qquad \qquad \qquad \quad & average\_class\_size,	expenses\_sport\_and\_other\_cultural\_activities	  \\	
\end{tabular}\newline 
\begin{tabular}{c c}
\qquad \qquad \qquad \qquad \quad & expenses\_per\_school\_student	   \\	
\end{tabular}\newline 
\small {All educational features are a measure of highest completed education. Furthermore SHE, MHE, LHE are abbreviations of short-, medium- and long-cycle higher education}. 
\normalsize
\subsection{Log Analysis}


\subsection{Merging Data}
Pandas objects can be combined in different ways according to the nature of features in the dataset. Relational database style operations are based on linking keys together, thereby maintaining the relationship between the combined datasets\footnote{McKinney, W. (2018). \textit{Data Wrangling: Join, Combine and Reshape}. p. 231 }

The data collection and scraping process provided a total of 15 Datasets from Boliga, Hvorlangterder, DAWA, the Danish Police, Social- \& Indenrigsministeriet and Danmarks Statistik. This section will describe how each dataset was merged, using pandas relational database styled merge and join operations. 

\paragraph{Boliga.dk\newline}
An evaluation of the Boliga datasets features and their ability to support further data collection led to the utilization of the following features: 

-	[Longitude, Latitude]: Geographical placements of the properties

-	Municipality: a numeric code for municipalities in Denmark

The Boliga dataset acts as a master dataset and was joined or merged upon throughout the process, and acts as the “left” of all operations. The Longitude and Latitude features serve as specific coordinates for valued properties but are in few cases repetitive regarding different apartments from the same complex. The municipality code was used for translation purposes between the master dataset and other datasets.  

\paragraph{DAWA\newline}
The DAWA dataset was scraped with an input of a distinct municipality code from the master dataset, returning a dataframe of municipality names for each code. This dataframe was merged onto the Boliga dataframe as a many-to-one merge with municipality code as the merge keys. The scraped municipality names from DAWA was used as the merge key for all further merges of municipal data. 

\paragraph{Danmarks Statistik, Danish Police \& Social og Indenrigsministeriet\newline}
Of the municipality-based data collected, the merging of these to our master dataframe was subsequently performed identically. Every dataset contained a column for unique municipality names and values for the given feature of the dataset. A many-to-one left merge was performed with the master dataframe, using municipality names. The result being a master dataframe containing municipal specific features for properties. 

The Danish Police datasets contained totals for different categories of crimes reported within municipalities. These datasets were outer joined with an index set to contain municipality names. Afterwards, the values were added to each other, providing a total of reported crimes within each municipality. The police statistics webpage does not specify whether different types of reported crimes can relate to a single case, but we assessed that the number of crimes reported provides a meaningful feature either way. The dataset containing total reported crimes per municipality, was merged with the master dataset in the same fashion as previous municipality-based features. 

\paragraph{Hvorlangterder.dk\newline}
The hvorlangterder scrape provided location specific features, taking an input of Latitude and Longitude coordinates. The scraping function, which was created for returning location-based features, created a column for the row specific values. Therefore, a merge operation was not necessary, but could alternatively have been managed with a one-to-one inner-merge id. As few coordinates regarding apartment properties are repetitive, these are not suitable for merging upon. 

\subsection{Data Cleaning}

A few of the listed housing, have been listed for over a decade. These are considered outliers, which are overly priced considering their characteristics. The outliers are excluded, because they would otherwise only contribute to further bias of overestimation. The total number of excluded housing are approximately 4000.  

\subsection{Descriptive Statistics}
In this section, we will examine the more apparent patterns of our collected data. This is done to familiarize ourselves with the data, before we commence on the analysis.
\subsubsection{Key Statistics}
Some key statistical characteristic is presented in table 1:
\begin{table}[H]
\begin{center}
\caption{Key Statistics\label{time}}
\begin{tabular}{| c | c | c | c | c | c |} 
\hline
   & Price & \, Rooms \ & \ $m^2$ \ & Average Municipal Income \\ \hline
   Mean & 2,312,798.00 & \ 4.21 \  & \ 127.82 \ & 312,957.03 \\ \hline
   Std & 2,351,127.00 & 2.12 & 75.67 & 46,095.34 \\ \hline
   Min & 15,000.00 & 0 & 0 & 257,776 \\ \hline
   25\% & 985,000.00 & 3 & 82 & 290,973 \\ \hline
   50\% & 1,695,000.00 & 4 & 123 & 302,153 \\ \hline
   75\% & 2,895,000.00 & 5 & 167 & 318,745 \\ \hline
   Max & 85,000,000.00 & 50 & 2,390 & 583,331 \\ \hline
\end{tabular}
\end{center}
\end{table} 
Initially, it should be addressed that some of the properties has 0 rooms and consists of 0 $m^2$.  This is due to the fact that we have also included land on which housing has not yet been build. Examining the 25\%-quintile and the 75\%-quintile of the valuation prices, it becomes apparent that there are some substantial outliers both to the cheaper and to expensive side. This is also the case with rooms and $m^2$, where the highest values are sizably higher than the 75\%-quintile. 
It is worth noting, that there is quite a big difference between the lowest average municipal income of DKK 257,776. and the highest of DKK 583,331. This difference becomes further noteworthy when assessing the 75\%-quintile. The difference from the 75\%-quintile to the highest average municipal income is more than 4 times the difference of the 75\%-quintile and the lowest income. The income distribution is visualised in the following figure:
\begin{figure}[H]
  \centering
   \caption{}
   \includegraphics[width=\linewidth]{Box1.png} 
  \label{fig:}
\end{figure}
The boxplot illustrates that the distribution is heavily left-skewed, where some municipalities has a sizably higher average income than the rest of the Danish municipalities. 

\subsubsection{Prices in Municipalities}
We plot the average square meter valuation price in the different municipalities:
\begin{figure}[H]
  \centering
   \caption{}
   \includegraphics[width=\linewidth]{Box2.png} 
  \label{fig:}
\end{figure}
It becomes apparent that the distribution is quite left-skewed and that there are a few municipalities who’s price per square meter is much higher than the rest of the municipalities. Recalling the geoplot in figure 1, these expensive outliers are heavily concentrated around- and north of Copenhagen. 
The distribution looks similar to that of figure 2 (The previous boxplot), though we cannot declare any correlation between average municipal income and average municipal price per square meter. Nonetheless, it becomes apparent that the valuation price of property is highly discriminated by municipal factors. The scope of this assignment is exactly to examine these factors and attempt to use these to evaluate an unseen, out-of-sample property. 

\subsubsection{Property Type}
Another worthwhile consideration is that we have included all types of properties. It would be reasonable to assume that there is an average difference in valuation pricing depending on the type of property. Figure 4 displays the median valuation price for each type:
\begin{figure}[H]
  \centering
   \caption{}
   \includegraphics[width=\linewidth]{bar1.png} 
  \label{fig:}
\end{figure}
It is interesting to note that the most expensive properties are apartments as opposed to houses. This is not especially surprising though, as it is a well-established trend that real estate prices in major cities are skyrocketing. Refraining from delving deeper into to a discussion of global urbanization, we retain the fact that property type does have an apparently significant effect on valuation pricings on average. We will include the ‘type’ feature in our impending model training to control for this effect. 

\subsubsection{The Dataset}
The merged dataset contains 61.618 observations and 37 features.   The features are of both categorical and continuous measures. Skal dette ikke bare op i Merging data?\newline 


\section{Methods}
\subsection{Supervised Machine Learning}
The objective by applying Machine Learning is to train a model that are able to make predictions in near future or never seen data. By feeding a model with labeled data as well as data samples, ML will define the algorithms that predicts the best\footnote{Rashka, Sebastian; Mirjalli, Vahid; \textit{Python Machine Learning, Machine Learning and Deep Learning with Python, scikit-learn, and Tensorflow}. p. 3}.   This project, implements a ML regression prediction model to predict pricing of housing listed in near future.
\subsection{Fitting the model}
The potential problems of underfitting and overfitting should be assessed when fitting a model. A model is underfitted if it hardly captures the variation of the sample data. It is then said that the model has \textit{high bias}. A model is overfitted, when it is overly sensitive to the idiosyncrasy of the sample data and captures the variation in too great detail. This problem often comes with the introduction of a sizeable number of features. Overfit models are said to have \textit{high variance}\footnote{Rashka, Sebastian; Mirjalli, Vahid; \textit{Python Machine Learning, Machine Learning and Deep Learning with Python, scikit-learn, and Tensorflow}. p.73}. In both cases, the model will generalize poorly. A key step in defining a decent model in machine learning is to find an optimal bias-variance-balance, by tuning the complexity of one’s model. This is done through \textit{regularization}. In this project three different types of regularization are applied; Lasso, Ridge and Elastic net.   

\subsubsection{Lasso}
Regularization by Lasso, will penalize complexity of the model by the sum of the absolute value of the coefficients. This penalty will make the model less complex and more appropriate for prediction.  \footnote{Foster, Ian; Rayid Ghani Ron S. Jarmin, Frauke Kreuter, Julia Lane; \textit{Big Data in Social Sciences, A Practical Guide to Methods and Tools} p. 173}\, \footnote{Rashka, Sebastian; Mirjalli, Vahid; \textit{Python Machine Learning, Machine Learning and Deep Learning with Python, scikit-learn, and Tensorflow} p. 332}
\newline Lasso minimizes: $$L_{Lasso}(\hat{\beta}) = \left(\sum_{i=1}^{n} (y_i-\hat{y_i}(\beta))^2+\lambda\sum_{j=1}^{p}|\hat{\beta_j}|\right) \qquad s.t \quad \lambda \geq 0 $$
Another convenient attribute of the Lasso penalty is that some estimates are set equal to zero and thereby produce sparse models\footnote{Hal R. Varian. \textit{Big data: New tricks for econometrics}. Journal of Economic Perspectives. p.19}. Lasso thereby performs the feature selection automatic.   
\subsubsection{Ridge}
The ridge model penalizes with the sum of squared coefficients. Opposed to Lasso, Ridge do not force features to be omitted. Instead Ridge penalizes the parameter estimates and decreases them towards zero. Ridge minimizes:
$$L_{Ridge}(\hat{\beta}) = \left(\sum_{i=1}^{n} (y_i-\hat{y_i}(\beta))^2+\lambda\sum_{j=1}^{p}\hat{\beta_j}^2\right) \qquad s.t \quad \lambda \geq 0 $$ 


\subsubsection{Elastic-net}
The Elastic net combines the penalizing terms of Lasso and Ridge, with $\alpha$ defining the relative weights between Lasso and Ridge. Elastic net minimizes the function: 
$$L_{elasticnet}(\hat{\beta}) = \frac{\sum_{i=1}^{n}\left(y_i-\hat{y_i}(\beta)\right)^2}{2n} + \lambda\left(\frac{1-\alpha}{2}\sum_{j=1}^{p}\hat{\beta_j}^2+\alpha\sum_{j=1}^{p}|\hat{\beta_j}|\right)$$
$$0 \leq \alpha \leq 1 \quad \wedge \quad \lambda \geq 0$$

\subsection{Selecting Features}
In case regularization is not sufficient to cope with the overfitting of the model. Exclusion of features are a viable approach. With the number of scraped features in this project, taking into consideration. A recurring overfitting of the model would be likely. As a consequence a deliberate exclusion of features of interest will need to be carried out.    

\subsection{Optimizing the Hyperparameter}
To minimize the mean squared errors of our Lasso and Ridge regression we performed k-fold cross validation to optimize the hyperparameter $\lambda$. 
We split the data into a test set and a development set, consisting of respectively 20\% and 80\% of the total observations. Subsequently, we use k-fold cross-validation to randomly split the development set into k folds, where k-1 folds are used to train the model. The remaining fold is used to validate the model’s generalizability by calculating the mean squared errors of the trained model’s prediction of the left-out fold\footnote{Rashka, Sebastian; Mirjalli, Vahid; \textit{Python Machine Learning, Machine Learning and Deep Learning with Python, scikit-learn, and Tensorflow}. p.191}. This process is repeated k times and each time a new fold is left out for validation. Since we are working with a relatively large dataset we chose to split our data into 5 folds, and computed the average MSE for the 5 iterations. By using the k-fold cross-validation method we relieve ourselves of the concern that the estimation of our model’s performance is simply due to a lucky or unlucky split of the data. \newline
We performed this procedure for 12 different values of $\lambda$ spanning between \textcolor{red}{$10^(-4) and 10^4$}. We chose the value of $\lambda$ which yields the smallest average MSE over the 5 folds. 
We both calculated the optimal hyperparameters for a Ridge regression model, a Lasso regression model and an Elastic Net regression model. \textcolor{red}{The following table} shows the performance of the different models, when trained with their optimal hyperparameter and predicting the test data.

\subsection{Predictive Performance}
With the hyperparameters optimized, final model performance can be evaluated. Once more a cross-validation is carried out, which return the average performance-error of each model. By retraining the models on the complete training set and testing on the independent test set, performance measures are obtained\footnote{Rashka, Sebastian; Mirjalli, Vahid; \textit{Python Machine Learning, Machine Learning and Deep Learning with Python, scikit-learn, and Tensorflow}. p.192}. \newline The performances of the models are simply measured by MSE, RMSE and MAE. \newline
$$ MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2$$

$$ RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y_i})^2}$$
$$ MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i-\hat{y_i}|$$



\section{Analysis}
A number of features are excluded before the training of the model. The exclusion was determined, quite simply, by investigation of the correlation between the complete set of features and the valuation price. A list of the excluded features are found in appendix 11.0.1. The model used for ML are found in appendix 11.0.2.  

The initial 10-fold cross-validation to obtain the optimal values of the hyperparameters in each regularization yields.
\begin{table}[h!]
\begin{center}
\caption{Optimal hyperparameters\label{time}}
\begin{tabular}{ c  c  c } 
   $\lambda_{Lasso}$ & $\lambda_{Ridge}$ &  $\lambda_{Elastic Net}$ \\ \hline
   1 & 1 & 1 \\ \hline
\end{tabular}
\end{center}
\end{table} 
The prediction-error of each model  


\begin{table}[h!]
\begin{center}
\caption{Prediction Errors\label{time}}
\begin{tabular}{| c | c | c | c | c | c |} 
\hline
   & Lasso & \, Ridge \ & \ Elastic Net \ & OLS \\ \hline
   MSE & 60,103,955,188.37 & \ 55,667,264,851.14 \  & \ 85,824,334,267.16 \ & 2.848e+28 \\ \hline
  RMSE & 245,161.08 & 235,939.11 & 292,957.91 & 168,763,945,674,352.3 \\ \hline
   MAE & 47,126.43 & 43,630.55 & 69,582.56 & 2,637,476,978,906.39 \\ \hline
\end{tabular}
\end{center}
\end{table}



\section{Results}
\begin{figure}[H]
\centering
\caption{Predicted Valuation vs. Actual Valuation}
%\includegraphics[scale=0.4]{}
%\source{Own creation, with data from \href{https://www.boliga.dk}{Boliga.dk}}
\end{figure}


\section{Discussion}
\subsection{Data critique}
Another interesting prediction which could have been done using the same methods, would have been to predict selling prices. This could have been done simply by scraping data on sold housing instead. This could be of more value for private agents, whose main interest should be the selling price of their housing. %This approach would undoubtedly get a greater data basis.  needs to add time series trends 
\newline 
In a prediction-model like this it is near impossible to evade some form of omitted variable bias. A significant amount of potential important factors can not be acquired. For an example the view from the listed housing will for sure be of great impact of the valuation price. Another factor of interest could have been a evaluation of the condition of the housing, unfortunately the statement of property\footnote{red. Tilstandsrapport} are not publicly accessible. \newline
The prediction of the cooperative housing valuation %\footnote{http://housingpeople.dk/en/housing-guide/housing-types/cooperative-andelsbolig/},
are subject to significant bias, since the cooperative housing that enters the marked through a realtor often would be those with critical amount of undesirable characteristics. Like debt - which deflate the valuation accordingly.

\subsection{Model Limitations and Irrelevancy}
The necessity to exclude municipality dummies as regressors in the prediction models. Leaves the model to only separate municipalities by the socio-economic factors, which are subject to low variance. This makes distinction between municipality a lot more difficult as well as uncertain.   
Without the municipality dummies, the interpretation of the prediction results are more insecure. %Instead of predicting the valuation of a given estate, of a given type, in a given municipality or city. It predicts the valuation of a estate at a \textit{more or less} location in Denmark by type, only geographically controlled by distances to various conveniences. 
Geographical boundaries could have been limited way more, resulting in easier interpretation of predictions. Potentially by predicting only for a specific city or municipality. \\
If one intended to predict valuations country-wide more realistic, a model for each municipality could have been more satisfactory. Resulting in 98 separate models. The evident downside of this approach being the prospect of limitations in available data (some municipalities has very few listings).     
Another approach could have been to group cities which share characteristics. Ex. The three biggest cities, the provinces, the islands, the country-side towns.    


\section{Conclusion}
Even after regularization, and deliberate exclusion of features. Our model still suffers from overfitting. 
\section{An Ethical Overview}
The ethical principles of social research are anchored in the fundamental human rights, which are broadly formulated in the UN Declaration of Human Rights. Additional policies and declarations that codify principles of research ethics and the ethical treatment of research participants include the Nuremberg Code, the Helsinki Declaration, the Belmont Report, and the Menlo Report\footnote{Salganik, Matthew J. (2018) \textit{Bit by Bit Bit - Social research in the digital age.}}\, \footnote{European Commission (2018). \textit{Ethics in Social Sciences and the Humanities}}. These codes and addendums originate mostly in the biomedical field, though they encompass the central principles applied to all human research, which have led some academics to call for a Hippocratic oath for data scientists to safeguard against powerful new technologies under development in laboratories and tech firms\footnote{Rotblat, Joseph (1999) \textit{A Hippocratic Oath for Scientists}}\, \footnote{Sample, Ian (2019, Fri 16 Aug 2019) \textit{and tech specialists need Hippocratic oath, says academic.}}. This discussion is nothing new however, as a tentative reformulation of the Hippocratic oath was introduced by Karl Popper\footnote{Popper, Karl (1969) \textit{The Moral Responsibility of the Scientist}}, wherein he stressed the importance of professional responsibility, a critical mind, and an overriding loyalty towards the betterment of mankind.\newline
Matthew Salganik offer four principles deduced from the Belmont and Menlo Report that should guide the ethical deliberations of the researcher: 1) the respect for persons, that is individuals should be treated as autonomous and if circumstances require it individuals should be entitled to additional protections. 2) Beneficence stresses the importance of doing no harm and to maximize the possible benefits and minimizing any potential harms. 3) The principle of justice touches upon the importance of the distribution of burdens and benefits of the social scientist's research. This principle stress that is should not be a single stratum of society that bears the costs of the research while another stratum benefits. 4) The fourth and final principle is the respect for law and public interest, according to Salganik, the principle consists of two distinct elements, that is compliance to relevant laws and legal contracts and transparency-based accountability. It is worth noting that Popper’s tentative Hippocratic oath mirrors the first three principles put forth by Salganik, stressing the importance of the ethical conduct of the researcher.\newline
The need for rigid ethical standards within computational social science was made apparent by the Cambridge Analytica Scandal that broke on the 17th of March 2018. Where Steve Bannon could reveal that between 2013 and 2015 Cambridge Analytica had exploited a loophole in Facebook’s API which allowed the company to harvest profile data from 87 million Facebook users, without the user’s permission and use the harvested data to construct a massive targeted marketing database based on the user’s likes and interests\footnote{Vox.com (2018) \textit{The Cambridge Analytica Facebook scandal} [Online]}. Other examples of misuse of data acquired from Facebook include the Harvard-run experiment, where students' data was used to create new knowledge about how social networks form and how these networks and their actors' behavior co-evolve, and the emotional contagion experiment from 2012, where approximately 700,000 users were involved in a research experiment to examine the extent to which a person's emotions are affected by the emotions of the people they interact with (Salganik. 2018).\newline
From this it should be evident that clear ethical guidelines are required in order to protect the user's privacy from tech-savvy companies. To this end, the European Parliament introduced the General Data Protection Regulation (GDPR). With its seven overarching principles the GDPR seeks to formalize the procedures involved in the data processing and storing of sensitive and private information (CDRC\footnote{Consumer Data Research Center, UK  [CDRC] (2018) \textit{The General Data Protection Regulation \& Social Science Research} [Online]:}). These principles include and expand upon the principles found in the Belmont and Menlo Report. The most important consideration, however, must be that even a dataset comprising tens of thousands of observations involve human beings who must be protected from adverse side-effects of the social research. There is considerable evidence that point to the fact that even in anonymized data sets it can be possible to backtrack an individual's identity. A researcher must therefore be mindful of the mosaic effect if the dataset combines large amount of data from various sources (European Commission 2018).

\newpage
\section{Litterature}
\ Foster, Ian; Rayid Ghani Ron S. Jarmin, Frauke Kreuter, Julia Lane; \textit{Big Data in Social Sciences, A Practical Guide to Methods and Tools}, CRS Press 2017 \newline

Rashka, Sebastian; Mirjalli, Vahid; \textit{Python Machine Learning, Machine Learning and Deep Learning with Python, scikit-learn, and Tensorflow}, $2^{nd}$editon, Packt Publishing 2017. \newline

Varian. Hal R. \textit{Big data: New tricks for econometrics}. Journal of Economic Perspectives, 28(2):3–28, 2014. \newline

Consumer Data Research Center, UK  [CDRC] (2018) \textit{The General Data Protection Regulation \& Social Science Research} [Online]: \href{https://ec.europa.eu/research/participants/data/ref/h2020/other/hi/h2020_ethics-soc-science-humanities_en.pdf}{ec.europa.eu/research/participants/data/ref/h2020/other/hi/h2020\_ethics-soc-science-humanities\_en.pdf} [Accessed on 25/08/2019]\newline

European Commission (2018). \textit{Ethics in Social Sciences and the Humanities} [Online]: \href{https://ec.europa.eu/research/participants/data/ref/h2020/other/hi/h2020_ethics-soc-science-humanities_en.pdf}{https://ec.europa.eu/research/participants/data/ref/h2020/other/hi/h2020\_ethics-soc-science-humanities\_en.pdf} [Accessed on 25/08/2019] \newline

Popper, Karl (1969) \textit{The Moral Responsibility of the Scientist}. Encounter, March 1969, pp. 52-56 [Online]: \href{http://www.unz.com/print/Encounter-1969mar-00052}{http://www.unz.com/print/Encounter-1969mar-00052} [Accessed on 25/08/2019]\newline

Rotblat, Joseph (1999) \textit{A Hippocratic Oath for Scientists}. Science, November 1999: Vol. 286, Issue 5444, pp. 1475 [Online]: \href{https://science.sciencemag.org/content/286/5444/1475.full}{https://science.sciencemag.org/content/286/5444/1475.full} [Accessed on 25/08/2019] DOI: 10.1126/science.286.5444.1475 \newline
 
Salganik, Matthew J. (2018) \textit{Bit by Bit Bit - Social research in the digital age.} Princeton, NJ: Princeton University Press.\newline

Sample, Ian (2019, Fri 16 Aug 2019) \textit{and tech specialists need Hippocratic oath, says academic.} The Guardian [Online]: \href{https://www.theguardian.com/science/2019/aug/16/mathematicians-need-doctor-style-hippocratic-oath-says-academic-hannah-fry} [Accessed on 25/08/2019]\newline

Vox.com (2018) \textit{The Cambridge Analytica Facebook scandal} [Online]: \href{https://www.vox.com/2018/4/10/17207394/cambridge-analytica-facebook-zuckerberg-trump-privacy-scandal} [Accessed on 26/08/2019] \newline
 
Boligagruppen.dk (2019) \textit{Om boliga gruppen} [Online]: \href{https://www.boligagruppen.dk}{www.boligagruppen.dk}: [Accessed on 27/08/2019] \newline

Shiab, Nael (2015) \textit{On the Ethics of Web Scraping and Data Journalism. Global Investigative Journalism Network.} [Online]: \href{https://gijn.org/2015/08/12/on-the-ethics-of-web-scraping-and-data-journalism/}{gijn.org/2015/08/12/on-the-ethics-of-web-scraping-and-data-journalism/}: [Accessed on 27/08/2019] \newline

McKinney, W. (2018). 'Data Wrangling: Join, Combine and Reshape'. In W. McKinney, \textit{Python for Data Analysis}. Sebastopol: O'Reilly Media Inc. \newline

Athey, Susan (2018) 'The Impact of Machine Learning on Economics' in Ajay Agrawal et al. (eds) \textit{The Economics of Artificial Intelligence: An Agenda} (2019). Chicago: University of Chicago Press. \newline

Einav, Liran and Jonathan Levin (2014) \textit{Economics in the age of big data}. Science, vol. 346, November. \newline

Grimmer, Justin (2015) \textit{We are all social scientists now: how big data, machine learning, and causal inference work together}. PS: Political Science \& Politics, vol. 48, January: 80-83. \newline

Lazer, David and Jason Radford (2017) \textit{Data ex Machina. Introduction to Big Data. Annual Review of Sociology}, vol. 43, August. \newline

Mullainathan, Sendhil, and Jann Spiess (2017) \textit{Machine Learning: An Applied Econometric Approach}. Journal of Economic Perspectives, vol. 31 (2): 87-106. \newline

Anderson, Chris (2008) \textit{The end of theory: The data deluge makes the scientific method obsolete}. Wired, 16-07 [Online] \href{https://www.wired.com/2008/06/pb-theory/}{www.wired.com/2008/06/pb-theory/} [Accessed on 27/08/2019]

%Varian, Hal R. (2014) Big Data: New Tricks for Econometrics. Journal of Economic Perspectives. Vol. 28, April: 3-27 \newline

\newpage
\section{Appendixs}
\subsubsection{}
Excluded Features:\newline
municipality, lotSize, unemployment, Total\_reported\_crime, Socioeconomic\_index, expenses\_per\_school\_student, expenses\_sport\_and\_other\_cultural\_activities, forest\_distance, coast\_distance, isForeclosure, owners\_expense
\subsubsection{}
The model used for prediction
$$ \hat{Y_i}=\hat{\beta_i}\textbf{X}_i+\hat{\epsilon_i}$$
$$\textbf{X}_i^T=\begin{bmatrix}
           basementSize \\
           buildYear \\
           rooms \\
           size \\
           primarySchool\_educ \\
           high\_school\_educ \\
           vocational\_educ \\
           SHE \\
           MHE \\
           bachelors\_degree \\
           LHE \\
           avg\_municipal\_income\_2017 \\
           Population\_in\_urban\_development \\
           Socioeconomic\_index \\
           lake\_distance\\
           doctor\_distance \\
           supermarket\_distance \\
           school\_distance \\
           daycare\_distance \\
           hospital\_distance \\
           train\_distance \\
           pharmacy\_distance \\
           library\_distance \\
         \end{bmatrix}$$
\end{document}